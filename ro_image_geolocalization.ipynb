{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torchmetrics kaggle"
      ],
      "metadata": {
        "id": "oW3_OMRD5NbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "8q9B1B45XHvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "JWw9AzDAXQlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d andreisaceleanu/romania-streetview"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snoS0dEVXabk",
        "outputId": "771b6877-991a-43bd-e9cc-8e24270d7ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading romania-streetview.zip to /content\n",
            "100% 3.59G/3.59G [02:58<00:00, 22.8MB/s]\n",
            "100% 3.59G/3.59G [02:58<00:00, 21.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Define the path to your zip file\n",
        "file_path = '/content/romania-streetview.zip'\n",
        "\n",
        "# Unzip the file to a specific destination\n",
        "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')"
      ],
      "metadata": {
        "id": "Fh0OX0VcX1J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics.pairwise import haversine_distances as hsine\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "from enum import Enum\n",
        "from math import e\n",
        "from tqdm import tqdm\n",
        "from torchmetrics import Accuracy, Precision, Recall, F1Score, MetricCollection\n",
        "from transformers import CLIPImageProcessor, CLIPVisionModel"
      ],
      "metadata": {
        "id": "tTb64Dv27Isn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing & loading"
      ],
      "metadata": {
        "id": "6NBZynde6A68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "\n",
        "    def __init__(self, fname_list, label_file, transform, mode=0, model_id=\"openai/clip-vit-base-patch32\", **kwargs):\n",
        "        super(ImageDataset, self).__init__(**kwargs)\n",
        "        self.fname_list = fname_list\n",
        "        self.transform = transform\n",
        "        with open(label_file, \"r\") as fin:\n",
        "            self.label_data = json.load(fin)\n",
        "\n",
        "        # self.region_map = {\n",
        "        #     0: [14, 38, 11, 23, 10, 7, 25, 9, 31],\n",
        "        #     1: [19, 36, 16, 2, 30, 39, 17, 20],\n",
        "        #     2: [41, 18, 3, 40, 29, 24, 35, 6],\n",
        "        #     3: [15, 21, 8, 28, 34, 5, 13, 0, 22],\n",
        "        #     4: [27, 12, 37, 1, 4, 32, 33, 26]\n",
        "        # }\n",
        "        # self.region = {}\n",
        "        # for k,v in self.region_map.items():\n",
        "        #     d = {elem:k for elem in v}\n",
        "        #     self.region.update(d)\n",
        "\n",
        "        self.name2idx = {k:idx for idx, k in enumerate(sorted(list(self.label_data.keys())))}\n",
        "        if \"clip\" not in model_id:\n",
        "            self.image_merge = self._concat_images if mode == 0 else self._stack_images\n",
        "        else:\n",
        "            self.processor = CLIPImageProcessor.from_pretrained(model_id)\n",
        "            self.image_merge = self._clip_images\n",
        "\n",
        "    def _clip_images(self, fnames):\n",
        "        return self.processor(\n",
        "            [\n",
        "                torch.tensor(cv2.imread(elem)).permute(2,0,1)\n",
        "                for elem in fnames\n",
        "            ],\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    def _concat_images(self, fnames):\n",
        "        return torch.cat(\n",
        "            [\n",
        "                self.transform(torch.tensor(cv2.imread(elem)).permute(2,0,1))\n",
        "                for elem in fnames\n",
        "            ],\n",
        "            dim=0\n",
        "        )\n",
        "\n",
        "    def _stack_images(self, fnames):\n",
        "        return torch.stack(\n",
        "            [\n",
        "                self.transform(torch.tensor(cv2.imread(elem)).permute(2,0,1))\n",
        "                for elem in fnames\n",
        "            ],\n",
        "            dim=1\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.fname_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        curr_fname = self.fname_list[idx]\n",
        "        admin1, admin2, loc_idx = curr_fname.split(os.sep)[-3:]\n",
        "        geo_coords = torch.tensor(\n",
        "            list(map(float, self.label_data[admin1][admin2][int(loc_idx)].split(\",\")))\n",
        "        )\n",
        "        location = os.path.abspath(self.fname_list[idx])\n",
        "\n",
        "        fnames = [\n",
        "            os.path.join(location, elem)\n",
        "            for elem in sorted(os.listdir(location)) if elem.endswith(\"jpg\")\n",
        "        ]\n",
        "        sample = self.image_merge(fnames)\n",
        "\n",
        "        # class_id = self.region[self.name2idx[admin1]]\n",
        "        class_id = self.name2idx[admin1]\n",
        "        return sample, class_id, geo_coords\n",
        "\n",
        "def get_geocell_centroids(train_paths, label_data, num_classes):\n",
        "\n",
        "    centroids = np.zeros((num_classes, 2))\n",
        "    cnts = defaultdict(lambda: 0)\n",
        "    name2idx = {k:idx for idx, k in enumerate(sorted(list(label_data.keys())))}\n",
        "\n",
        "    for elem in train_paths:\n",
        "        county, city, idx = elem.split(os.sep)[-3:]\n",
        "        lat, lon = list(map(float, label_data[county][city][int(idx)].split(\",\")))\n",
        "        centroids[name2idx[county]] += np.array([lat, lon], dtype=np.float32)\n",
        "        cnts[name2idx[county]] += 1\n",
        "\n",
        "    for class_idx in range(num_classes):\n",
        "        centroids[class_idx] /= cnts[class_idx]\n",
        "\n",
        "    return centroids\n",
        "\n",
        "\n",
        "def get_dataloaders(db_root: str, eval_splits=(0.1, 0.1), batch_size=2, random_seed=42, mode=0, model_id=\"openai/clip-vit-base-patch32\", **kwargs):\n",
        "\n",
        "    image_root = os.path.join(db_root, \"images\")\n",
        "    counties = sorted(os.listdir(image_root))\n",
        "\n",
        "    train_paths, val_paths, test_paths = [], [], []\n",
        "    train_percent = 1 - sum(eval_splits)\n",
        "    test_percent = eval_splits[1]\n",
        "\n",
        "    label_file = os.path.join(db_root, \"locations.json\")\n",
        "    with open(label_file, \"r\") as fin:\n",
        "        label_data = json.load(fin)\n",
        "\n",
        "    num_classes = 0\n",
        "    for county in tqdm(counties):\n",
        "        num_classes += 1\n",
        "        cities = os.listdir(os.path.join(image_root, county))\n",
        "        for city in cities:\n",
        "            if city in label_data[county]:\n",
        "                locations = [\n",
        "                    os.path.join(image_root, county, city, elem)\n",
        "                    for elem in os.listdir(os.path.join(image_root, county, city))\n",
        "                ]\n",
        "                cnt_locations = len(locations)\n",
        "                np.random.seed(random_seed)\n",
        "                np.random.shuffle(locations)\n",
        "                train_paths.extend(locations[:int(cnt_locations*train_percent)])\n",
        "                val_paths.extend(locations[int(cnt_locations*train_percent):-int(cnt_locations*test_percent)])\n",
        "                test_paths.extend(locations[-int(cnt_locations*test_percent):])\n",
        "\n",
        "    centroids = get_geocell_centroids(train_paths, label_data, num_classes)\n",
        "\n",
        "    transform = v2.Compose(\n",
        "        [\n",
        "            v2.Lambda(lambd=lambda x: x[...,:512,:512]),\n",
        "            v2.RandomCrop((128,128), pad_if_needed=True),\n",
        "            v2.ToDtype(torch.float32, scale=True),\n",
        "            v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    )\n",
        "    train_dataset = ImageDataset(fname_list=train_paths, label_file=label_file, mode=mode, model_id=model_id, transform=transform)\n",
        "    val_dataset = ImageDataset(fname_list=val_paths, label_file=label_file, mode=mode, model_id=model_id, transform=transform)\n",
        "    test_dataset = ImageDataset(fname_list=test_paths, label_file=label_file, mode=mode, model_id=model_id, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    return (train_loader, val_loader, test_loader), centroids, num_classes"
      ],
      "metadata": {
        "id": "xGnOjK_Y6D2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_root = \"/content/geo_dbv2\"\n",
        "(train_loader, val_loader, test_loader), centroids, num_classes = get_dataloaders(db_root, batch_size=32,mode=\"1\",model_id=\"cnn\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfbRwNDK6Ror",
        "outputId": "014e15a1-ac92-4d5d-c42c-292b01ffead5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 42/42 [00:00<00:00, 299.86it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model architectures"
      ],
      "metadata": {
        "id": "aon2iC4K6dJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Pretrain(Enum):\n",
        "    NO_PRETRAIN = 0\n",
        "    PATH = 1\n",
        "    URL = 2\n",
        "\n",
        "def hook_fn(module, input, output, name, d):\n",
        "    d.update({name: output})"
      ],
      "metadata": {
        "id": "utwrf1s566V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GeoFinderV1(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes=42,\n",
        "        in_channels=6,\n",
        "        pretrained=Pretrain.NO_PRETRAIN,\n",
        "        pretrain_url=\"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\",\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "\n",
        "        super(GeoFinderV1, self).__init__(**kwargs)\n",
        "        self.adapter = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=3,\n",
        "            kernel_size=1,\n",
        "            stride=1\n",
        "        )\n",
        "        self.backbone = torchvision.models.resnet50()\n",
        "        if pretrained == Pretrain.URL:\n",
        "            state_dict = torch.hub.load_state_dict_from_url(pretrain_url)\n",
        "            self.backbone.load_state_dict(state_dict)\n",
        "\n",
        "        in_features = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Linear(\n",
        "            in_features=in_features,\n",
        "            out_features=num_classes,\n",
        "            bias=False\n",
        "        )\n",
        "        self.layers = dict(self.backbone.named_modules())\n",
        "        self.intermediates = {}\n",
        "\n",
        "    def enable_layer(self, layer_name):\n",
        "\n",
        "        self.layers[layer_name].register_forward_hook(lambda module, input, output, name=layer_name, d=self.intermediates: hook_fn(module, input, output, name, d))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(self.adapter(x))"
      ],
      "metadata": {
        "id": "cKshDbvZ6f1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GeoFinderV2(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes=42,\n",
        "        in_channels=3,\n",
        "        features_base = 32,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        super(GeoFinderV2, self).__init__(**kwargs)\n",
        "\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, features_base, kernel_size=4, stride=2, padding=1), # 32x64x64\n",
        "            nn.LeakyReLU(0.2),\n",
        "            self._block(features_base, features_base * 2, 4, 2, 1), # 64x32x32\n",
        "            self._block(features_base * 2, features_base * 4, 4, 2, 1), # 128x16x16\n",
        "            self._block(features_base * 4, features_base * 8, 4, 2, 1), # 256x8x8\n",
        "            # self._block(features_base * 8, features_base * 16, 4, 2, 1), # 512x4x4\n",
        "            # self._block(features_base * 16, features_base * 32, 4, 2, 1), # 1024x4x4\n",
        "        )\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(features_base * 8, features_base * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(features_base * 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                padding=padding,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels, affine=True),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "    def _get_embedding(self, x):\n",
        "        features = self.backbone(x)\n",
        "        feature_embeds = torch.mean(features, dim=(-2,-1))\n",
        "        return feature_embeds\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) == 4:\n",
        "            embeds = self._get_embedding(x)\n",
        "        else:\n",
        "            embeds = torch.stack(\n",
        "                [self._get_embedding(x[:,:,idx,:,:]) for idx in range(x.shape[2])],\n",
        "                dim=-1\n",
        "            ).mean(dim=-1)\n",
        "\n",
        "        return self.head(embeds)"
      ],
      "metadata": {
        "id": "uniSO8Be60_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GeoFinderV3(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes=42,\n",
        "        in_channels=3,\n",
        "        features_base = 32,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        super(GeoFinderV3, self).__init__(**kwargs)\n",
        "\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, features_base, kernel_size=3, stride=1, padding=1), # 32x2x128x128\n",
        "            nn.LeakyReLU(0.2),\n",
        "            self._block(features_base, features_base * 2, (3,4,4), (1,2,2), 1), # 64x2x64x64\n",
        "            self._block(features_base * 2, features_base * 4, (3,4,4), (1,2,2), 1), # 128x2x32x32\n",
        "            self._block(features_base * 4, features_base * 8, 4, 2, 1), # 256x1x16x16\n",
        "        )\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(features_base * 8, features_base * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(features_base * 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "\n",
        "        return nn.Sequential(\n",
        "            nn.Conv3d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                padding=padding,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm3d(out_channels, affine=True),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "    def _get_embedding(self, x):\n",
        "        features = self.backbone(x)\n",
        "        feature_embeds = torch.mean(features, dim=(-3, -2, -1))\n",
        "        return feature_embeds\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self._get_embedding(x)\n",
        "        return self.head(embeds)"
      ],
      "metadata": {
        "id": "zP3QsnU_BCoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GeoFinderV4(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes=42,\n",
        "        model_id=\"openai/clip-vit-base-patch32\",\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        super(GeoFinderV4, self).__init__(**kwargs)\n",
        "\n",
        "        self.backbone = CLIPVisionModel.from_pretrained(model_id)\n",
        "\n",
        "        self.head = nn.Linear(768, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        views = torch.chunk(x[\"pixel_values\"],chunks=2,dim=1)\n",
        "\n",
        "        embeds = torch.stack(\n",
        "            [self.backbone(pixel_values=view[:,0]).pooler_output for view in views],\n",
        "            dim=-1\n",
        "        ).mean(dim=-1)\n",
        "        return self.head(embeds)"
      ],
      "metadata": {
        "id": "xtARxHjJC9pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and evaluation"
      ],
      "metadata": {
        "id": "aW63FnDI8mhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, centroids, metrics_group, tau=50, device=\"cpu\"):\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    model.train()\n",
        "    for inputs, labels, coords in tqdm(loader):\n",
        "        # inputs = {k:v.to(device) for k, v in inputs.items()}\n",
        "        optimizer.zero_grad()\n",
        "        inputs = inputs.to(device)\n",
        "        out = F.softmax(model(inputs), dim=-1)\n",
        "        logits = -torch.log(out+1e-10) # B x 42\n",
        "\n",
        "        input_coords = coords.detach().numpy()\n",
        "        a = 6371 * hsine(np.deg2rad(input_coords), np.deg2rad(centroids)) # B x 42\n",
        "        b = np.diag(6371 * hsine(np.deg2rad(centroids[labels.detach().numpy()]), np.deg2rad(coords)))[:, np.newaxis] # B x 1\n",
        "        weights = torch.tensor(e**(-(a - b) / tau)).detach().to(device) # B x 42\n",
        "\n",
        "\n",
        "        loss = torch.mean(torch.sum(logits * weights, dim=1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        labels = labels.to(device)\n",
        "        metrics_group.update(out, labels)\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(loader), metrics_group.compute()\n",
        "\n",
        "def evaluate(model, loader, centroids, metrics_group, tau=50, device=\"cpu\"):\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, coords in tqdm(loader):\n",
        "            # inputs = {k:v.to(device) for k, v in inputs.items()}\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            out = F.softmax(model(inputs), dim=-1)\n",
        "            logits = -torch.log(out+1e-10) # B x 42\n",
        "\n",
        "            input_coords = coords.detach().numpy()\n",
        "            a = 6371 * hsine(np.deg2rad(input_coords), np.deg2rad(centroids)) # B x 42\n",
        "            b = np.diag(6371 * hsine(np.deg2rad(centroids[labels.detach().numpy()]), np.deg2rad(coords)))[:, np.newaxis] # B x 1\n",
        "            weights = torch.tensor(e**(-(a - b) / tau)).detach().to(device) # B x 42\n",
        "\n",
        "            loss = torch.mean(torch.sum(logits * weights, dim=1))\n",
        "\n",
        "            labels = labels.to(device)\n",
        "            metrics_group.update(out, labels)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(loader), metrics_group.compute()"
      ],
      "metadata": {
        "id": "54uDtO9-9SMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = GeoFinderV3(num_classes=num_classes)\n",
        "# for param in model.backbone.parameters():\n",
        "#     param.requires_grad = False\n",
        "model = model.to(device)\n",
        "optimizer = optim.AdamW([elem for elem in model.parameters() if elem.requires_grad], lr=5e-3, weight_decay=0.0)\n",
        "epochs = 2\n",
        "\n",
        "metrics_group = MetricCollection(\n",
        "    [\n",
        "        Accuracy(task=\"multiclass\", num_classes=num_classes),\n",
        "        Precision(task=\"multiclass\", num_classes=num_classes),\n",
        "        Recall(task=\"multiclass\", num_classes=num_classes),\n",
        "        F1Score(task=\"multiclass\", num_classes=num_classes)\n",
        "    ]\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "PNzkO7fs8n6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_score = None\n",
        "best_state_dict = None\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    train_loss, train_metrics = train(\n",
        "        model,\n",
        "        train_loader,\n",
        "        optimizer,\n",
        "        centroids,\n",
        "        metrics_group,\n",
        "        tau=100,\n",
        "        device=device\n",
        "    )\n",
        "    metrics_group.reset()\n",
        "\n",
        "    valid_loss, valid_metrics = evaluate(\n",
        "        model,\n",
        "        val_loader,\n",
        "        centroids,\n",
        "        metrics_group,\n",
        "        tau=100,\n",
        "        device=device\n",
        "    )\n",
        "    metrics_group.reset()\n",
        "\n",
        "    valid_f1 = valid_metrics[\"MulticlassF1Score\"].cpu().item()\n",
        "    if (best_score is None) or (valid_f1 > best_score):\n",
        "        best_score = valid_f1\n",
        "        best_state_dict = model.state_dict()\n",
        "\n",
        "    print(\"Train\")\n",
        "    print(f\"Loss: {train_loss:.3f}\", end=\", \")\n",
        "    print(\", \".join([f\"{k}: {v.cpu().item() * 100:.2f}\" for k, v in train_metrics.items()]))\n",
        "\n",
        "    print(\"Validation\")\n",
        "    print(f\"Loss: {valid_loss:.3f}\", end=\", \")\n",
        "    print(\", \".join([f\"{k}: {v.cpu().item() * 100:.2f}\" for k, v in valid_metrics.items()]))\n",
        "    print()\n",
        "\n",
        "model.load_state_dict(best_state_dict)\n",
        "test_loss, test_metrics = evaluate(\n",
        "    model,\n",
        "    test_loader,\n",
        "    centroids,\n",
        "    metrics_group,\n",
        "    tau=50,\n",
        "    device=device\n",
        ")\n",
        "metrics_group.reset()\n",
        "\n",
        "print(\"Test\")\n",
        "print(f\"Loss: {test_loss:.3f}\", end=\", \")\n",
        "print(\", \".join([f\"{k}: {v.cpu().item() * 100:.2f}\" for k, v in test_metrics.items()]))"
      ],
      "metadata": {
        "id": "ky1QD08G9FMf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "be969c09-4879-4a4c-e4c4-492f48c73367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 735/735 [04:39<00:00,  2.63it/s]\n",
            "100%|██████████| 95/95 [00:31<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train\n",
            "Loss: 31.314, MulticlassAccuracy: 2.33, MulticlassPrecision: 2.33, MulticlassRecall: 2.33, MulticlassF1Score: 2.33\n",
            "Validation\n",
            "Loss: 31.205, MulticlassAccuracy: 2.38, MulticlassPrecision: 2.38, MulticlassRecall: 2.38, MulticlassF1Score: 2.38\n",
            "\n",
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 19/735 [00:07<04:37,  2.58it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-cb3d198267ec>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     train_loss, train_metrics = train(\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-831aa990385c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, centroids, metrics_group, tau, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m# inputs = {k:v.to(device) for k, v in inputs.items()}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_score = None\n",
        "best_state_dict = None\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    train_loss, train_metrics = train(\n",
        "        model,\n",
        "        train_loader,\n",
        "        optimizer,\n",
        "        centroids,\n",
        "        metrics_group,\n",
        "        tau=50,\n",
        "        device=device\n",
        "    )\n",
        "    metrics_group.reset()\n",
        "\n",
        "    valid_loss, valid_metrics = evaluate(\n",
        "        model,\n",
        "        val_loader,\n",
        "        centroids,\n",
        "        metrics_group,\n",
        "        tau=50,\n",
        "        device=device\n",
        "    )\n",
        "    metrics_group.reset()\n",
        "\n",
        "    valid_f1 = valid_metrics[\"MulticlassF1Score\"].cpu().item()\n",
        "    if (best_score is None) or (valid_f1 > best_score):\n",
        "        best_score = valid_f1\n",
        "        best_state_dict = model.state_dict()\n",
        "\n",
        "    print(\"Train\")\n",
        "    print(f\"Loss: {train_loss:.3f}\", end=\", \")\n",
        "    print(\", \".join([f\"{k}: {v.cpu().item() * 100:.2f}\" for k, v in train_metrics.items()]))\n",
        "\n",
        "    print(\"Validation\")\n",
        "    print(f\"Loss: {valid_loss:.3f}\", end=\", \")\n",
        "    print(\", \".join([f\"{k}: {v.cpu().item() * 100:.2f}\" for k, v in valid_metrics.items()]))\n",
        "    print()\n",
        "\n",
        "model.load_state_dict(best_state_dict)\n",
        "test_loss, test_metrics = evaluate(\n",
        "    model,\n",
        "    test_loader,\n",
        "    centroids,\n",
        "    metrics_group,\n",
        "    tau=50,\n",
        "    device=device\n",
        ")\n",
        "metrics_group.reset()\n",
        "\n",
        "print(\"Test\")\n",
        "print(f\"Loss: {test_loss:.3f}\", end=\", \")\n",
        "print(\", \".join([f\"{k}: {v.cpu().item() * 100:.2f}\" for k, v in test_metrics.items()]))"
      ],
      "metadata": {
        "id": "te2sGWbaZgcT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a12a3fc0-279a-4135-f1a5-8b632761bf13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 735/735 [11:45<00:00,  1.04it/s]\n",
            "100%|██████████| 95/95 [01:30<00:00,  1.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train\n",
            "Loss: 14.256, MulticlassAccuracy: 9.83, MulticlassPrecision: 9.83, MulticlassRecall: 9.83, MulticlassF1Score: 9.83\n",
            "Validation\n",
            "Loss: 14.003, MulticlassAccuracy: 12.21, MulticlassPrecision: 12.21, MulticlassRecall: 12.21, MulticlassF1Score: 12.21\n",
            "\n",
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 735/735 [11:47<00:00,  1.04it/s]\n",
            "100%|██████████| 95/95 [01:32<00:00,  1.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train\n",
            "Loss: 13.938, MulticlassAccuracy: 14.05, MulticlassPrecision: 14.05, MulticlassRecall: 14.05, MulticlassF1Score: 14.05\n",
            "Validation\n",
            "Loss: 13.911, MulticlassAccuracy: 15.54, MulticlassPrecision: 15.54, MulticlassRecall: 15.54, MulticlassF1Score: 15.54\n",
            "\n",
            "Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 735/735 [11:41<00:00,  1.05it/s]\n",
            "100%|██████████| 95/95 [01:29<00:00,  1.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train\n",
            "Loss: 13.903, MulticlassAccuracy: 14.68, MulticlassPrecision: 14.68, MulticlassRecall: 14.68, MulticlassF1Score: 14.68\n",
            "Validation\n",
            "Loss: 13.926, MulticlassAccuracy: 13.63, MulticlassPrecision: 13.63, MulticlassRecall: 13.63, MulticlassF1Score: 13.63\n",
            "\n",
            "Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 735/735 [11:39<00:00,  1.05it/s]\n",
            "100%|██████████| 95/95 [01:32<00:00,  1.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train\n",
            "Loss: 13.882, MulticlassAccuracy: 15.62, MulticlassPrecision: 15.62, MulticlassRecall: 15.62, MulticlassF1Score: 15.62\n",
            "Validation\n",
            "Loss: 13.705, MulticlassAccuracy: 15.68, MulticlassPrecision: 15.68, MulticlassRecall: 15.68, MulticlassF1Score: 15.68\n",
            "\n",
            "Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 735/735 [11:41<00:00,  1.05it/s]\n",
            "100%|██████████| 95/95 [01:31<00:00,  1.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train\n",
            "Loss: 13.769, MulticlassAccuracy: 16.60, MulticlassPrecision: 16.60, MulticlassRecall: 16.60, MulticlassF1Score: 16.60\n",
            "Validation\n",
            "Loss: 14.051, MulticlassAccuracy: 14.06, MulticlassPrecision: 14.06, MulticlassRecall: 14.06, MulticlassF1Score: 14.06\n",
            "\n",
            "Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 735/735 [11:49<00:00,  1.04it/s]\n",
            "100%|██████████| 95/95 [01:37<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train\n",
            "Loss: 13.763, MulticlassAccuracy: 17.03, MulticlassPrecision: 17.03, MulticlassRecall: 17.03, MulticlassF1Score: 17.03\n",
            "Validation\n",
            "Loss: 13.862, MulticlassAccuracy: 14.19, MulticlassPrecision: 14.19, MulticlassRecall: 14.19, MulticlassF1Score: 14.19\n",
            "\n",
            "Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 18/735 [00:18<12:36,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-9fcd9f216895>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     train_loss, train_metrics = train(\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-831aa990385c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, centroids, metrics_group, tau, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# inputs = {k:v.to(device) for k, v in inputs.items()}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1e-10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B x 42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/feature_extraction_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;31m# cast and send to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                 \u001b[0mnew_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x1T97PLCSb_e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}